# sLLM模型综合性能排名 (Small LLM Overall Performance Ranking)

以下排名基于所有测试维度的标准化分数（0-10分制）计算得出。对于偏见、幻觉、毒性等负向指标，分数越低越好，已转换为标准化分数。

| 排名 | 模型 | 参数大小 | 文件大小 | 平均分 | 总分 | 完成测试数 | 偏见测试 (Bias) | 上下文相关性测试 (Contextual Relevancy) | 忠实度测试 (Faithfulness) | 幻觉测试 (Hallucination) | 摘要测试 (Summarization) | 毒性测试 (Toxicity) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | cogito:3b | 3B | 2.2GB | 7.72 | 46.34 | 6 | 9.49 | 2.00 | 4.85 | 10.00 | 10.00 | 10.00 |
| 2 | qwen2.5:3b | 3B | 1.9GB | 6.54 | 39.25 | 6 | 10.00 | 8.40 | 10.00 | 0.00 | 2.76 | 8.09 |
| 3 | gemma3:1b | 1B | 815MB | 5.46 | 32.78 | 6 | 6.62 | 10.00 | 4.56 | 2.81 | 3.07 | 5.71 |
| 4 | llama3.2:3b | 3B | 2.1GB | 5.41 | 32.47 | 6 | 5.82 | 5.36 | 2.01 | 5.31 | 6.94 | 7.02 |
| 5 | llama3.2:1b | 1B | 1GB | 5.27 | 31.60 | 6 | 2.20 | 7.35 | 8.03 | 2.81 | 3.63 | 7.57 |
| 6 | deepseek-r1:1.5b | 1.5B | 1.1GB | 4.90 | 29.41 | 6 | 9.91 | 8.78 | 4.78 | 5.94 | 0.00 | 0.00 |
| 7 | qwen3:1.7b | 1.7B | 1.2GB | 3.96 | 23.73 | 6 | 9.29 | 1.55 | 5.15 | 2.81 | 3.27 | 1.66 |
| 8 | smollm2:1.7b | 1.7B | 1.3GB | 2.31 | 13.85 | 6 | 0.00 | 0.00 | 0.00 | 9.06 | 1.17 | 3.62 |
